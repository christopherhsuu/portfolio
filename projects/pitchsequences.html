<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Pitch Sequences — Christopher Hsu</title>
    <link href="../styles.css" rel="stylesheet">
  </head>
  <body>
    <header class="site-header">
      <div class="container header-inner">
        <div class="brand">
          <a href="/" class="site-title">Christopher Hsu</a>
          <p class="tagline">Data Science &amp; Sabermetrics</p>
        </div>
        <nav aria-label="Main navigation">
          <ul class="nav-list">
            <li><a href="/">Home</a></li>
            <li><a href="/about.html">About</a></li>
          </ul>
        </nav>
      </div>
    </header>

    <main class="container section">
      <h1>Pitch Sequences</h1>
      <p class="muted">An interactive visualization of pitch sequences, pitch types, and swing outcomes. The app helps explore sequencing patterns and batter responses by zone and count.</p>

      <div style="margin-top:20px; display:flex; gap:12px; align-items:center;">
  <a class="launch-big" href="https://pitchsequence.streamlit.app/" target="_blank" rel="noopener noreferrer">Launch (Live)</a>
        <a class="btn ghost" href="https://github.com/christopherhsuu/pitchsequence/tree/main" target="_blank" rel="noopener noreferrer">Code</a>
      </div>

      <section style="margin-top:28px;">
        <h2>Purpose</h2>
        <p class="muted">PitchSequence recommends next-pitch candidates for a pitcher-vs-batter matchup by estimating expected run impact at the play level. It merges model-driven scores with batter-specific matchup aggregates to produce actionable candidate rankings.</p>

        <h2>Quick start</h2>
        <ol>
          <li>Clone the repo, create a Python 3.13 virtualenv, and install dependencies.</li>
          <li>Run preprocessing and feature engineering to create `data/processed/*.parquet` (see scripts).</li>
          <li>Train or load a saved `artifacts/runvalue_model.pkl`, then open the Streamlit app to try the stepwise at-bat UI.</li>
        </ol>

        <h2>Data preparation (what I did)</h2>
        <ol>
          <li>Collected raw Statcast play-by-play CSVs and name mapping files and stored them in `data/raw/`.</li>
          <li>Ran `src/preprocess.py` to clean rows, normalize pitch types, and standardize names.</li>
          <li>Executed `src/feature_engineering.py` to compute per-play features and battery-vs-pitchtype aggregates; saved outputs as Parquet.</li>
          <li>Applied exposure filters and winsorization to ensure stable aggregates.</li>
        </ol>

        <h2>Feature engineering (what I built)</h2>
        <ol>
          <li>Extracted plate coordinates (plate_x/plate_z), last-pitch deltas, and zone buckets.</li>
          <li>Computed batter_pitchtype aggregates (wOBA, whiff rate, run value) by grouping processed records.</li>
          <li>Encoded categorical features and prepared inputs for the run-value pipeline.</li>
        </ol>

        <h2>Modeling &amp; training</h2>
        <ol>
          <li>Loaded `data/processed/features.parquet` and `data/processed/runvalue.parquet`.</li>
          <li>Built a scikit-learn pipeline for encoding, imputation, scaling, and the estimator (GradientBoosting or RandomForest).</li>
          <li>Used time-aware train/validation/test splits to avoid leakage and tuned hyperparameters via grid/random search.</li>
          <li>Saved the final pipeline to `artifacts/runvalue_model.pkl` for use in the app or server endpoints.</li>
        </ol>

        <h2>Deployment &amp; usage</h2>
        <p class="muted">The Streamlit app loads the saved pipeline and scores candidate pitches for a given state. Predictions are converted into candidate probabilities by adding predicted run-value to the current run expectancy and applying a softmax over negative expected runs. The repo also includes a Vercel serverless endpoint that returns recommendations as JSON for programmatic use.</p>

        <h2>Challenges &amp; next steps</h2>
        <p class="muted">Name normalization, sparse batter-pitchtype exposures, and ensuring robust recommendations are primary challenges. The repo mitigates these with name-mapping, minimum exposure thresholds, and blending heuristics. Future work: richer batter features, ensemble stacking, periodic retraining, and a dashboard for evaluation of recommended sequences on holdout data.</p>
      </section>

      <section style="margin-top:22px;">
        <h2>Diagnostics &amp; common pitfalls</h2>
        <p class="muted">Common issues when running PitchSequence and how to quickly identify them.</p>

        <div style="display:flex; gap:12px; align-items:flex-start; margin-top:12px;">
          <img src="../images/diag-pitchsequence.png" alt="diagnostic screenshot" style="width:260px; border-radius:8px; box-shadow:0 8px 20px rgba(0,0,0,0.5);">
          <div>
            <strong>Name normalization mismatches</strong>
            <p class="muted">If recommendations look odd for a given batter, check name mapping joins. Use an inner-join audit to find unmatched keys:</p>
            <pre style="background:rgba(255,255,255,0.02); padding:8px; border-radius:6px; color:var(--muted-text);"># audit unmatched names
unmatched = raw_df[~raw_df['batter_name'].isin(name_map['clean_name'])]['batter_name'].unique()
print('Unmatched batter names sample:', unmatched[:10])
            </pre>

            <strong>Sparse exposure</strong>
            <p class="muted">Low counts for a batter-pitchtype pair can make run-value estimates unstable. Consider blending with league averages or applying a shrinkage estimator.</p>
          </div>
        </div>
      </section>

    </main>

    <footer class="site-footer">
      <div class="container">
        <p class="muted">© 2025 Christopher Hsu</p>
      </div>
    </footer>
  </body>
</html>